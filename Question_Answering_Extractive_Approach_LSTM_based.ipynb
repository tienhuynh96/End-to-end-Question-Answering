{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tienhuynh96/End-to-end-Question-Answering/blob/main/Question_Answering_Extractive_Approach_LSTM_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Question answering Extractive Approach with LSTM-based***\n",
        "\n",
        "input_text = question + ' sep ' + context\n",
        "\n",
        "Backbone: LSTM"
      ],
      "metadata": {
        "id": "YS8ctbaGbieN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import libraries and create temp datasets**"
      ],
      "metadata": {
        "id": "5C8jXzW55vTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "4GAgtobPvWRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = [\n",
        "    {\n",
        "        'context': 'My name is AIVN and I am from Vietnam.',\n",
        "        'question': 'What is my name?',\n",
        "        'answer': 'AIVN'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I love painting and my favorite artist is Vicent Van Gogh.',\n",
        "        'question': 'What is my favorite activity?',\n",
        "        'answer': 'painting'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I am studying computer science at the University of Tokyo.',\n",
        "        'question': 'What am I Studying?',\n",
        "        'answer': 'computer science'\n",
        "    },\n",
        "    {\n",
        "        'context': 'My favorite book is \"To kill a Mockingbird\" by Harper Lee.',\n",
        "        'question': 'What is my favorite book?',\n",
        "        'answer': '\"To kill a Mockingbird\"'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I have a pet dog named Max who loves to play fetch',\n",
        "        'question': 'What is the name of my pet?',\n",
        "        'answer': 'Max'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I was born in Paris, but now I live in New york City',\n",
        "        'question': 'Where do I live now?',\n",
        "        'answer': 'New York City'\n",
        "    }\n",
        "    # {\n",
        "    #     'context': '',\n",
        "    #     'question': '',\n",
        "    #     'answer': ''\n",
        "    # },\n",
        "\n",
        "]\n",
        "\n",
        "data_size = len(qa_dataset)\n",
        "data_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQnZjl0g05aD",
        "outputId": "ee867555-cba5-4052-c25b-71a5fbc07dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(qa_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cBe6FiG4-h0",
        "outputId": "a9ed5e08-f899-4e67-b02f-73f09cc6a93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6,)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Build vocabulary**"
      ],
      "metadata": {
        "id": "eknboL3K4953"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Create a function to yield list of tokens\n",
        "# This yield function is required in function \"build_vocab_from_iterator\"\n",
        "# Get data from item context and question in data\n",
        "def yield_tokens(data):\n",
        "  for item in data:\n",
        "    yield tokenizer(item['context'] + ' <sep> ' + item['question'])\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(qa_dataset),\n",
        "    specials = ['<unk>','<pad>','<bos>','<eos>','<sep>']\n",
        ")\n",
        "\n",
        "# Set default index for this vocab is 'unk' = 0, when the unknow word is replace the 'unk'\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "# Build vocab (stoi mean string to index)\n",
        "vocab.get_stoi()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lx7PjY5f05RQ",
        "outputId": "0c67f04c-5e3a-42b6-da9a-65b61e63a225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 24,\n",
              " ',': 25,\n",
              " 'pet': 21,\n",
              " 'who': 61,\n",
              " 'gogh': 39,\n",
              " 'the': 23,\n",
              " 'fetch': 37,\n",
              " 'play': 52,\n",
              " 'van': 56,\n",
              " 'now': 19,\n",
              " 'was': 59,\n",
              " 'a': 14,\n",
              " 'name': 13,\n",
              " 'aivn': 27,\n",
              " 'i': 5,\n",
              " 'studying': 22,\n",
              " 'and': 15,\n",
              " 'where': 60,\n",
              " '<unk>': 0,\n",
              " 'favorite': 11,\n",
              " 'by': 32,\n",
              " 'artist': 28,\n",
              " 'live': 18,\n",
              " '<eos>': 3,\n",
              " 'harper': 40,\n",
              " 'dog': 36,\n",
              " 'loves': 45,\n",
              " '<pad>': 1,\n",
              " 'computer': 34,\n",
              " '.': 10,\n",
              " 'born': 30,\n",
              " 'is': 6,\n",
              " 'my': 7,\n",
              " 'book': 16,\n",
              " 'science': 53,\n",
              " 'of': 20,\n",
              " '<bos>': 2,\n",
              " '<sep>': 4,\n",
              " 'what': 9,\n",
              " 'am': 12,\n",
              " 'named': 48,\n",
              " 'at': 29,\n",
              " 'but': 31,\n",
              " 'in': 17,\n",
              " 'from': 38,\n",
              " 'tokyo': 54,\n",
              " 'city': 33,\n",
              " 'kill': 42,\n",
              " 'lee': 43,\n",
              " 'love': 44,\n",
              " '?': 8,\n",
              " 'do': 35,\n",
              " 'max': 46,\n",
              " 'mockingbird': 47,\n",
              " 'york': 62,\n",
              " 'new': 49,\n",
              " 'painting': 50,\n",
              " 'paris': 51,\n",
              " 'university': 55,\n",
              " 'have': 41,\n",
              " 'vicent': 57,\n",
              " 'activity': 26,\n",
              " 'vietnam': 58}"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "text = 'I love AIVN'\n",
        "tokens = tokenizer(text)\n",
        "tokens = [vocab[token] for token in tokens]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T_3gVuW8o8N",
        "outputId": "6f9271d7-5927-44bb-80f1-9b8516802da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 44, 27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Create vectorize function**"
      ],
      "metadata": {
        "id": "9bL-nt5cdGXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad an truncate\n",
        "MAX_SEQ_LENGTH = 22\n",
        "PAD_IDX = vocab['<pad>']\n",
        "\n",
        "# define pad and truncate function\n",
        "def pad_and_truncate(input_ids, max_seq_len):\n",
        "  if len(input_ids) > max_seq_len:\n",
        "    input_ids = input_ids[:max_seq_len]\n",
        "  elif len(input_ids) < max_seq_len:\n",
        "    input_ids += [PAD_IDX] * (max_seq_len - len(input_ids))\n",
        "\n",
        "  return input_ids"
      ],
      "metadata": {
        "id": "Im8xAlhH05Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# Pad an truncate\n",
        "MAX_SEQ_LENGTH = 22\n",
        "PAD_IDX = vocab['<pad>']\n",
        "\n",
        "text = 'I love AIVN'\n",
        "tokens = tokenizer(text)\n",
        "tokens = [vocab[token] for token in tokens]\n",
        "print(tokens)\n",
        "padded_tokens = pad_and_truncate(tokens, MAX_SEQ_LENGTH)\n",
        "print(padded_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLR0YIn4Aah_",
        "outputId": "c8003a94-1623-4206-efd3-d4f064cba7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 44, 27]\n",
            "[5, 44, 27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vectorize function\n",
        "def vectorize(question, context, answer):\n",
        "  input_text = question + ' <sep> ' + context\n",
        "  input_ids = [vocab[token] for token in tokenizer(input_text)]\n",
        "  input_ids = pad_and_truncate(input_ids, MAX_SEQ_LENGTH)\n",
        "\n",
        "  answer_ids = [vocab[token] for token in tokenizer(answer)]\n",
        "  # Find the start position of answer in input text\n",
        "  start_positions = input_ids.index(answer_ids[0])\n",
        "  # Calculate the end position of answer in input text\n",
        "  end_positions = start_positions + len(answer_ids) - 1\n",
        "\n",
        "  input_ids = torch.tensor(input_ids, dtype = torch.long)\n",
        "  start_positions = torch.tensor(start_positions, dtype = torch.long)\n",
        "  end_positions = torch.tensor(end_positions, dtype = torch.long)\n",
        "\n",
        "  return input_ids, start_positions, end_positions\n",
        "\n"
      ],
      "metadata": {
        "id": "1FbItdwuAae_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check function\n",
        "MAX_SEQ_LENGTH = 22\n",
        "input_ids, start_positions, end_positions  = vectorize(\n",
        "    qa_dataset[0]['question'],\n",
        "    qa_dataset[0]['context'],\n",
        "    qa_dataset[0]['answer']\n",
        ")\n",
        "\n",
        "print(input_ids)\n",
        "print(start_positions)\n",
        "print(end_positions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9vAHd1ZzG7H",
        "outputId": "f672400a-65bd-4cbf-cb4c-33c7ba2a810f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 9,  6,  7, 13,  8,  4,  7, 13,  6, 27, 15,  5, 12, 38, 58, 10,  1,  1,\n",
            "         1,  1,  1,  1])\n",
            "tensor(9)\n",
            "tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Create datasets**"
      ],
      "metadata": {
        "id": "gehgPajV0bkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self,data):\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    item = self.data[idx]\n",
        "    question_text = item['question']\n",
        "    context_text = item['context']\n",
        "    answer_text = item['answer']\n",
        "\n",
        "    input_ids, start_positions, end_positions = vectorize(question_text, context_text, answer_text)\n",
        "\n",
        "    return   input_ids, start_positions, end_positions\n"
      ],
      "metadata": {
        "id": "OSfGuItGAacY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode\n",
        "# Define decode function: convert id to token\n",
        "def decode(input_ids):\n",
        "  return ' '.join([vocab.lookup_token(token) for token in input_ids])"
      ],
      "metadata": {
        "id": "orMbBrZW05L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = QADataset(qa_dataset)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 2, shuffle=True)"
      ],
      "metadata": {
        "id": "YIvsl5cV2ofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOrkC6mf2oaY",
        "outputId": "feafaa34-537d-42d3-b138-4772857eaf27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[60, 35,  5, 18, 19,  8,  4,  5, 59, 30, 17, 51, 25, 31, 19,  5, 18, 17,\n",
              "          49, 62, 33,  1],\n",
              "         [ 9,  6,  7, 11, 26,  8,  4,  5, 44, 50, 15,  7, 11, 28,  6, 57, 56, 39,\n",
              "          10,  1,  1,  1]]),\n",
              " tensor([18,  9]),\n",
              " tensor([20,  9])]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(next(iter(train_loader))[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TcopPHWxguzQ",
        "outputId": "822724b7-0a9f-4767-877d-8969e591d23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is the name of my pet ? <sep> i have a pet dog named max who loves to play fetch <pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for batch in train_loader:\n",
        "  input_ids, start_positions, end_positions = batch\n",
        "  print (f'{decode(input_ids[1])} \\n {input_ids[1]} \\n {start_positions[1]} \\n  {end_positions[1]} \\n')\n",
        "  text = tokenizer(decode(input_ids[1]))\n",
        "  kq =\n",
        "\n",
        "  print(decode(input_ids[1]).split()[start_positions[1]: end_positions[1]+1])\n",
        "\n",
        "\n",
        "    #   context_tokens = tokenizer(context)\n",
        "    # predicted_answer_tokens = context[start_position: end_position + 1]\n",
        "    # predicted_answer = ' '.join(predicted_answer_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "Wx7U2EeVhzjB",
        "outputId": "7fd0c9e8-b26e-4b06-d16c-bb651460f8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-116-722dfffa30fe>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-116-722dfffa30fe>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    kq =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Create models**"
      ],
      "metadata": {
        "id": "BZbi1dL43n-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This model use Bi-LSTM\n",
        "class QAModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers):\n",
        "    super(QAModel, self).__init__()\n",
        "    # Get embedding for input\n",
        "    self.input_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Use bi-LSTM\n",
        "    # concat question embed, context embed\n",
        "    self.lstm = nn.LSTM(\n",
        "        embedding_dim, hidden_size,\n",
        "        num_layers    = n_layers,\n",
        "        batch_first   = True,\n",
        "        bidirectional = True\n",
        "    )\n",
        "\n",
        "    self.start_linear = nn.Linear(hidden_size * 2, 1)\n",
        "    self.end_linear = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "  def forward(self, text):\n",
        "    input_embedded = self.input_embedding(text)\n",
        "\n",
        "    lstm_out, _ = self.lstm(input_embedded) #(output: bs, seq_len, hidden_units)\n",
        "\n",
        "    # squeeze(-1) is removes the singleton dimension at the last position of the tensor\n",
        "    # Logits (bs, 22, 1) => (bs, 22)\n",
        "    start_logits = self.start_linear(lstm_out).squeeze(-1)\n",
        "    end_logits = self.end_linear(lstm_out).squeeze(-1)\n",
        "\n",
        "    return start_logits, end_logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CkuZ4FP2oS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_SIZE = 128\n",
        "VOCAB_SIZE = len(vocab)\n",
        "N_LAYERS = 2\n",
        "\n",
        "model = QAModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS)\n",
        "\n",
        "input = torch.randint(0, 10, size=(1, 10)) # (size = 1, 10 =>  batch, sequence length)\n",
        "print(input.shape)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  start_logits, end_logits = model(input)\n",
        "\n",
        "print(start_logits.shape)"
      ],
      "metadata": {
        "id": "02I12tR_A5Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6ef249-0446-4f34-c04e-92fedb85173f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n",
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vkwbmFoA498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Training models**"
      ],
      "metadata": {
        "id": "-kt0WOaMATJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-3\n",
        "EPOCHS = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "SX9ym2Ujp_nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.train()\n",
        "for _ in range(EPOCHS):\n",
        "  for idx, (input_ids, start_positions, end_positions) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    start_logits, end_logits = model(input_ids)\n",
        "\n",
        "    start_loss = criterion(start_logits, start_positions)\n",
        "    end_loss = criterion(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    print(total_loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfuG8ILK6ta2",
        "outputId": "65ee44e6-45d5-4d12-e46d-396096639c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.088099241256714\n",
            "3.088763952255249\n",
            "3.076913356781006\n",
            "2.989846706390381\n",
            "3.003505229949951\n",
            "2.935987710952759\n",
            "2.9173898696899414\n",
            "2.842987060546875\n",
            "2.7384867668151855\n",
            "2.728424072265625\n",
            "2.640113353729248\n",
            "2.4949307441711426\n",
            "2.428497791290283\n",
            "2.2472143173217773\n",
            "2.2086668014526367\n",
            "2.0647835731506348\n",
            "1.758601188659668\n",
            "1.7666841745376587\n",
            "1.4358246326446533\n",
            "1.2988473176956177\n",
            "1.5729725360870361\n",
            "1.1286625862121582\n",
            "1.2045471668243408\n",
            "0.9661874771118164\n",
            "1.1532917022705078\n",
            "0.7436903119087219\n",
            "0.7646517157554626\n",
            "0.6815716028213501\n",
            "0.7659229636192322\n",
            "0.6826803684234619\n",
            "0.6893978118896484\n",
            "0.42751944065093994\n",
            "0.6361417770385742\n",
            "0.4645516872406006\n",
            "0.46301761269569397\n",
            "0.3367407023906708\n",
            "0.33089935779571533\n",
            "0.21035964787006378\n",
            "0.24283066391944885\n",
            "0.2306172400712967\n",
            "0.14449387788772583\n",
            "0.1378726363182068\n",
            "0.1386144459247589\n",
            "0.07431986927986145\n",
            "0.10917755961418152\n",
            "0.06932646036148071\n",
            "0.12645593285560608\n",
            "0.012542685493826866\n",
            "0.09758003056049347\n",
            "0.009313525632023811\n",
            "0.030084727331995964\n",
            "0.05382411181926727\n",
            "0.015683269128203392\n",
            "0.019839324057102203\n",
            "0.01673090271651745\n",
            "0.026742950081825256\n",
            "0.016987141221761703\n",
            "0.020870545879006386\n",
            "0.015324552543461323\n",
            "0.007147438824176788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Test**"
      ],
      "metadata": {
        "id": "cCuwXzI5BdVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  sample = qa_dataset[2]\n",
        "  context, question, answer = sample.values()\n",
        "  input_ids, start_positions, end_positions = vectorize(question, context, answer)\n",
        "\n",
        "  # Add batch\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "\n",
        "  # Compute start and eng logits\n",
        "  start_logits, end_logits = model(input_ids)\n",
        "\n",
        "  # Compute offset is number tokens of question and <sep>\n",
        "  offset = len(tokenizer(question)) + 1\n",
        "  start_position = torch.argmax(start_logits, dim=1).numpy()[0]\n",
        "  end_position = torch.argmax(end_logits, dim=1).numpy()[0]\n",
        "\n",
        "  # minus to offset\n",
        "  start_position -=offset\n",
        "  end_position -=offset\n",
        "\n",
        "  # set condition to start and end position\n",
        "  start_position = max(start_position, 0)\n",
        "  end_position = min(end_position, len(tokenizer(context)) - 1)\n",
        "\n",
        "  # Check start and position\n",
        "  if end_position >= start_position:\n",
        "    # Extract the predicted answer span\n",
        "    context_tokens = tokenizer(context)\n",
        "    predicted_answer_tokens = context_tokens[start_position: end_position + 1]\n",
        "    predicted_answer = ' '.join(predicted_answer_tokens)\n",
        "\n",
        "  else:\n",
        "    predicted_answer = ' '\n",
        "\n",
        "  print(f'Context: {context}')\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Start position: {start_position}')\n",
        "  print(f'End position: {end_position}')\n",
        "  print(f'Prediction: {predicted_answer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yDGCdVP6tYY",
        "outputId": "7d01bec3-67b0-4f60-9e53-772f20df2347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: I am studying computer science at the University of Tokyo.\n",
            "Question: What am I Studying?\n",
            "Start position: 3\n",
            "End position: 4\n",
            "Prediction: computer science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4XggfVd6tVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHRAhmqO6tTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "392EEkxe6tQe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}